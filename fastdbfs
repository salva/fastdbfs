#!/usr/bin/env python3

import sys
import asyncio
import aiohttp
import cmd
import configparser
import os.path
import urllib.parse
import traceback
import time
import shlex
import pathlib
import base64
import subprocess
import tempfile
import json
import zlib
import progressbar

class FileInfo():

    @staticmethod
    def from_json(json):
        return FileInfo(json["is_dir"],
                        json["file_size"],
                        json["modification_time"],
                        json["path"])
    
    def __init__(self, is_dir, size, mtime, abspath):
        self._is_dir = is_dir
        self._size = size
        self._mtime = mtime
        self._abspath = abspath

    def path(self):
        return os.path.basename(self._abspath)

    def is_dir(self):
        return self._is_dir

    def mtime(self):
        return self._mtime

    def size(self):
        return self._size

    def type(self):
        return "dir" if self._is_dir else "file"

class APIError(Exception):
    def __init__(self, error_code, message):
        super().__init__(message)
        self.error_code = error_code
    
class Disconnected():
    def __init__(self):
        pass
    
    def __getattr__(self, method):
        raise Exception("open must be called first!")

    def prompt(self):
        return "*disconnected* "


class DBFS():

    def __init__(self, id, host, cluster_id, token,
                 workers=8, chunk_size=1048576, max_retries=10,
                 error_delay=10, error_delay_increment=10):
        self.id = id
        self.host = host
        self.cluster_id = cluster_id
        self.token = token
        self.path = "/"
        self.workers = workers
        self.chunk_size = chunk_size
        self.max_retries = max_retries
        self.error_delay = error_delay
        self.error_delay_increment = error_delay_increment

        self.semaphore = asyncio.Semaphore(workers)
        
        # print(f"host: {self.host}, cluster: {self.cluster_id}, token: {self.token}")

    def check(self):
        self._assert_dir(".")
        
    def _resolve(self, path):
        return os.path.normpath(os.path.join(self.path, path))

    def prompt(self):
        return f"{self.id}:{self.path}$ "
    
    def cd(self, path):
        path = self._resolve(path)
        self._assert_dir(path)
        self.path = path
        
    async def _unpack_response(self, response):
        status = response.status
        ct = response.headers['content-type']
        if ct != "application/json":
            raise Exception(f"Unexpected response received from API, Content-type: {ct}, HTTP status: {status}")

        try:
            data = await response.json()
        except OSError as ex:
            # connection errors and alike should be retried
            raise ex
        except Exception as ex:
            raise Exception("Unable to retrieve API response") from ex

        if status == 200:        
            return data

        try:
            error_code = data["error_code"]
            message = data["message"]
        except:
            raise Exception("Unexpected response received from API, HTTP status {status}")

        raise APIError(error_code, message)

    async def _async_control(self, cb):
        retries = 0
        while True:
            async with self.semaphore:
                try:
                    return await cb()
                except OSError as ex:
                    if retries > self.max_retries:
                        raise ex
                except Exception as ex:
                    print(f"Exception caught: {ex}")
                    raise ex
                # We keep the semaphore while we sleep in order to
                # reduce the pressure on the remote side.
                await asyncio.sleep(self.error_delay + self.error_delay_increment * retries)
                retries += 1

    async def _async_get(self, session, end_point, **params):
        async def cb():
            url = urllib.parse.urljoin(self.host, end_point)
            headers = { "Authorization": "Bearer " + self.token }
            #print(f"session: {session}\nurl: {url}\nparams: {params}\nheaders: {headers}")
            async with session.get(url,
                                   params=params,
                                   headers=headers) as response:
                return await self._unpack_response(response)
        return await self._async_control(cb)


    async def _async_post(self, session, end_point, **data):
        async def cb():
            url = urllib.parse.urljoin(self.host, end_point)
            headers = { "Authorization": "Bearer " + self.token,
                        "Content-Type": "application/json" }
            load = json.dumps(data)
            async with session.post(url,
                                    headers=headers,
                                    data=load) as response:
                return await self._unpack_response(response)

        return await self._async_control(cb)

    async def _async_call_with_session(self, cb, *params, **kwparams):
        async with aiohttp.ClientSession() as session:
            return await cb(session, *params, **kwparams)
    
    async def _async_get_status(self, session, path):
        r = await self._async_get(session, "api/2.0/dbfs/get-status", path=self._resolve(path))
        return FileInfo.from_json(r)

    def _run_with_session(self, task, *args, **kwargs):
        task_with_session = self._async_call_with_session(task, *args, **kwargs)
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(task_with_session)

    def _simple_get(self, end_point, **params):
        return self._run_with_session(self._async_get, end_point, **params)

    def _simple_post(self, end_point, **params):
        return self._run_with_session(self._async_post, end_point, **params)
            
    def get_status(self, path):
        return self._run_with_session(self._async_get_status, path)

    def ls(self, path):
        path = self._resolve(path)
        fi = self.get_status(path)
        if fi.is_dir():
            r = self._simple_get("api/2.0/dbfs/list", path=path)
            if "files" in r:
                return [FileInfo.from_json(e) for e in r["files"]]
            return []
        else:
            return [fi]

    def rm(self, path):
        self._simple_post("api/2.0/dbfs/delete",
                          path=self._resolve(path),
                          recursive=False)
        
    def _assert_dir(self, path):
        if not self.get_status(path).is_dir():
            raise Exception("Not a directory")

    async def _async_create(self, session, path, overwrite=False):
        path = self._resolve(path)
        out = await self._async_post(session,
                                     "api/2.0/dbfs/create",
                                     path=path, overwrite=overwrite)
        return int(out["handle"])

    async def _async_add_block(self, session, handle, block):
        return await self._async_post(session,
                                      "api/2.0/dbfs/add-block",
                                      data=base64.standard_b64encode(block).decode('ascii'),
                                      handle=handle)        

    async def _async_close(self, session, handle):
        return await self._async_post(session,
                                      "api/2.0/dbfs/close",
                                      handle=handle)
    
    async def _async_put_from_file(self, session, infile, target, size=None, overwrite=False, update_cb=None):
        target = self._resolve(target)

        handle = await self._async_create(session, target, overwrite)
        try:
            bytes_copied = 0
            while True:
                chunk = infile.read(self.chunk_size)
                chunk_len = len(chunk)
                if chunk_len == 0:
                    break

                await self._async_add_block(session, handle, chunk)

                bytes_copied += chunk_len
                if update_cb:
                    await update_cb(size, bytes_copied)

            await self._async_close(session, handle)

            fi = await self._async_get_status(session, target)
            if fi.size() != bytes_copied:
                raise Exception(f"corrupted copy detected, copied: {bytes_copied}, remote: {fi.size()}, expected: {size}")
            return fi
        except Exception as ex:
            try: await self._async_rm(target)
            except: pass
            raise ex
    
    def put_from_file(self, infile, target, size=None, overwrite=False):

        with progressbar.DataTransferBar() as bar:
            if size is not None:
                bar.max_value = size 
        
            async def update_cb(size, bytes_read):
                bar.update(bytes_read)
                
            return self._run_with_session(self._async_put_from_file,
                                          infile, target, size, overwrite,
                                          update_cb)

    def _new_swarm(self, workers, queue_max_size):

        queue = asyncio.Queue(maxsize = queue_max_size)
    
        async def worker(ix):
            # print(f"worker {ix} started")
            
            async with aiohttp.ClientSession() as session:
                while True:
                    # print(f"worker {ix}  waiting for tasks")
                    task = await queue.get()
                    if task is None:
                        return
                    (cb, response_queue, kwargs) = task
                    try:
                        value = await cb(session, **kwargs)
                        res = (value, None)
                    except Exception as ex:
                        res = (None, ex)
                    if response_queue:
                        await response_queue.put(res)

        async def end():
            for _ in range(workers):
                await queue.put(None)
                        
        swarm = asyncio.gather(*[worker(ix) for ix in range(workers)])
        return (swarm, queue, end)

    async def _queue_task(self, queue, task, response_queue=None, **kwargs):
        return await queue.put((task, response_queue, kwargs))

    def _unwrap_response(self, res):
        (value, ex) = res
        if ex is not None:
            raise ex
        return value

    async def _async_get_chunk(self, session, path, offset, length, out):
        # print(f"processing chunk at {offset}")
        remaining = length
        while remaining > 0:
            r = await self._async_get(session,
                                      "api/2.0/dbfs/read",
                                      path=path, offset=offset, length=length)
        
            bytes_read = r["bytes_read"]
            if bytes_read == 0 or bytes_read > remaining:
                raise Exception("Invalid data response")

            chunk = base64.standard_b64decode(r["data"])
            if len(chunk) != bytes_read:
                raise Exception("Invalid data response, size is not as promised")

            out.seek(offset)            
            out.write(chunk)
            remaining -= bytes_read
            offset += bytes_read
        # print(f"chunk of {length} bytes copied at offset {offset - length}")
        return length

    async def _async_get_to_file(self, queue, src, out, update_cb=None):
        src = self._resolve(src)
        fi = await self._async_call_with_session(self._async_get_status, src)
        size = fi.size()

        response_queue = asyncio.Queue()
        active_tasks = 0 # we count the request we have queued
        bytes_copied = 0
        offset = 0

        try:
            while True:
                while True:
                    try:
                        res = response_queue.get_nowait()
                    except asyncio.QueueEmpty:
                        break
                    active_tasks -= 1
                    bytes_copied += self._unwrap_response(res)
                    if update_cb:
                        await update_cb(size, bytes_copied)
                    
                next_offset = min(offset + self.chunk_size, size)
                length = next_offset - offset
                if length <= 0:
                    break

                await self._queue_task(queue,
                                       self._async_get_chunk,
                                       response_queue=response_queue,
                                       path=src, offset=offset, length=length,
                                       out=out)
                active_tasks += 1
                offset = next_offset

            while active_tasks > 0:
                res = await response_queue.get()
                active_tasks -= 1
                bytes_copied += self._unwrap_response(res)
                if update_cb:
                    await update_cb(size, bytes_copied)

        finally:
            for _ in range(active_tasks):
                try: await response_queue.get()
                except: pass

        return fi
            

    def get_to_file(self, src, out):

        (swarm, queue, end) = self._new_swarm(self.workers, self.workers * 2)

        with progressbar.DataTransferBar() as bar:
            async def update_cb(size, read):
                bar.max_value = size
                bar.update(read)

            result = None
            async def control():
                nonlocal result
                try:
                    result = await self._async_get_to_file(queue, src, out, update_cb)
                finally:
                    await end()
                
            loop = asyncio.get_event_loop()
            loop.run_until_complete(asyncio.gather(control(), swarm))
            return result

class CLI(cmd.Cmd):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

        self._init_cfg()
        self._dbfs = Disconnected()
        self._set_prompt()
        self._debug = True

    def cfg(self, section, key, default=None):
        try:
            return self._cfg[section][key]
        except:
            if default is not None:
                return default
            raise Exception(f"Configuration entry {key} missing in section {section}")

    def cfg_int(self, section, key):
        v = self.cfg(section, key)
        try:
            return int(v)
        except:
            raise Exception(f"Configuration entry {key} is not an integer")
        
    def _init_cfg(self):
        self._cfg = configparser.ConfigParser()
        self._cfg["fastdbfs"] = {
            "workers": 8,
            "chunk_size": 1048576,
            "max_retries": 10,
            "pager": "less",
            "error_delay": 10,
            "error_delay_increment": 10
        }

        home = os.path.expanduser("~")
        fns = [os.path.join(home, fn) for fn in (".databrickscfg", ".fastdbfs", ".config/fastdbfs")]
        self._cfg.read(fns)    
        
    def _tell_error(self, msg):
        _, ex, trace = sys.exc_info()
        print(f"{msg}: {ex}")
        if self._debug:
            print("Stack trace:")
            traceback.print_tb(trace)
        
    def do_open(self, arg):
        try:
            id, = self._parse(arg, "DEFAULT")
            dbfs = DBFS(id,
                        host = self.cfg(id, "host"),
                        cluster_id = self.cfg(id, "cluster_id"),
                        token = self.cfg(id, "token"),
                        chunk_size = self.cfg_int("fastdbfs", "chunk_size"),
                        workers = self.cfg_int("fastdbfs", "workers"),
                        max_retries = self.cfg_int("fastdbfs", "max_retries"))
            
            dbfs.check()
            self._dbfs = dbfs
        except:
            self._tell_error(f"Unable to open {arg}")

    def do_cd(self, arg):
        path, = self._parse(arg, "/")
        try:
            self._dbfs.cd(path)
        except:
            self._tell_error(f"{path}: unable to change dir")

    def _format_size(self, size):
        if (size >= 1073741824):
            return "%.1fG" % (size / 1073741824)
        if (size > 1024 * 1024):
            return "%.1fM" % (size / 1048576)
        if (size > 1024):
            return "%.1fK" % (size / 1024)
        return str(int(size))

    def _format_time(self, mtime):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mtime/1000))

    def do_ls(self, arg):
        try:
            path, = self._parse(arg, ".")
            # cols are type, size, mtime and path
            type_len = 3
            size_len = 1
            mtime_len = 1
            table = []
            for e in self._dbfs.ls(path):
                row = (e.type(),
                       self._format_size(e.size()),
                       self._format_time(e.mtime()),
                       e.path())

                # print(f"row: {row}")
                type_len = max(type_len, len(row[0]))
                size_len = max(size_len, len(row[1]))
                mtime_len = max(mtime_len, len(row[2]))
                table.append(row)

            fmt = "{:>"+str(type_len)+"} {:>"+str(size_len)+"} {:>"+str(mtime_len)+"} {}"
            for e in table:
                print(fmt.format(*e))

        except:
            self._tell_error(f"{arg}: unable to list directory")

    def do_lcd(self, arg):
        try:
            path, = self._parse(arg, os.path.expanduser("~"))
            os.chdir(path)
        except:
            self._tell_error(f"{arg}: unable to change dir")

    def do_lpwd(self, arg):
        self._parse(arg)
        try:
            print(os.getcwd())
        except:
            self._tell_error("getcwd failed")

    def _mkdir(self, path):
        pathlib.Path(path).mkdir(parents=True, exist_ok=True)

    def do_put(self, arg):
        try:
            src, target = self._parse(arg, ".", min=1)
            try:
                # We check whether the target exists and if it is a
                # directory.  If it is a directory we compose the name
                # using the src basename and check the target again.
                for first in (True, False):
                    fi = self._dbfs.get_status(target)
                    if first and fi.is_dir():
                        target = os.path.join(target, os.path.basename(src))
                    else:
                        break
            except Exception as ex:
                print(f"file not found at {target}: {ex}")
                # No file there, ok!
                pass
            else:
                raise Exception("File already exists")

            print(f"copying to {target}")

            size = os.stat(src).st_size
            
            with open(src, "rb") as infile:
                start = time.time()
                self._dbfs.put_from_file(infile, target, size=size)
                delta = max(1, time.time() - start)

        except:
            self._tell_error(f"{arg}: put failed")

    def _get_to_temp(self, src, prefix=".tmp-", suffix=None, **kwargs):
        try:
            if suffix is None:
                bn = os.path.basename(src)
                try: suffix = bn[bn.rindex("."):]
                except: suffix = ""

            (f, target) = tempfile.mkstemp(prefix=prefix, suffix=suffix, **kwargs)
            out = os.fdopen(f, "wb")
            self._dbfs.get_to_file(src, out)
            out.close()
            return target
        
        except Exception as ex:
            try: out.close()
            except: pass
            try: os.remove(tmp_fn)
            except: pass
            raise ex
            
    def do_get(self, arg):
        try:
            src, target = self._parse(arg, ".", min=1)

            if os.path.isdir(target):
                target = os.path.join(target, os.path.basename(src))
            if os.path.exists(target):
                raise Exception("file already exists")

            parent_dir, _ = os.path.split(target)
            self._mkdir(parent_dir)

            tmp_target = self._get_to_temp(src, prefix=".transferring-", suffix="", dir=parent_dir)
            os.rename(tmp_target, target)
                
        except:
            self._tell_error(f"{arg}: unable to retrieve remote file")

    def _get_and_call(self, src, cb):
        target = self._get_to_temp(src)
        try:
            cb(target)
        finally:
            try: os.remove(target)
            except: pass

    def do_rm(self, arg):
        try:
            path, = self._parse(arg, min=1)
            self._dbfs.rm(path)
            print("File removed");
        except:
            self._tell_error(f"{arg}: unable to remove remote file")

    def do_cat(self, arg):
        try:
            src, = self._parse(arg, min=1)
            def cb(fn):
                subprocess.run(["cat", "--", fn])
            self._get_and_call(arg, cb)
        except:
            self._tell_error(f"{arg}: unable to show file")
            
    def do_more(self, arg, pager=None):
        try:
            src, = self._parse(arg, min=1)
            if pager is None:
                pager = self.cfg("fastdbfs", "pager")
            def cb(fn):
                subprocess.run([pager, "--", fn])
            self._get_and_call(arg, cb)
        except:
            self._tell_error(f"{arg}: unable to show file")

    def do_edit(self, arg, editor=None):
        try:
            src, = self._parse(arg, min=1)
            if editor is None:
                editor = self.cfg("fastdbfs", "editor", default=os.environ.get("EDITOR", "vi"))

            def cb(tmp_fn):
                # in order to avoid race conditions we force the mtime
                # of the temporal file into the past
                the_past = int(time.time() - 2)
                os.utime(tmp_fn, times=(the_past, the_past))
                subprocess.run([editor, "--", tmp_fn])
                stat_after = os.stat(tmp_fn)
                if (stat_after.st_mtime_ns > the_past):
                    with open(tmp_fn, "rb") as infile:
                        self._dbfs.put_from_file(infile, src, size=stat_after.st_size, overwrite=True)
                else:
                    raise Exception(f"File was not modified!")

            self._get_and_call(arg, cb)
        except:
            self._tell_error(f"{arg}: edit failed")

    def do_mg(self, arg):
        self.do_edit(arg, editor="mg")

    def do_vi(self, arg):
        self.do_edit(arg, editor="vi")
            
    def do_less(self, arg):
        self.do_more(arg, pager="less")

    def do_batcat(self, arg):
        self.do_more(arg, pager="batcat")

    def do_shell(self, arg):
        os.system(arg)

    def do_EOF(self, arg):
        print("\nBye!")
        return True

    def do_exit(self, arg):
        print("Bye!")
        return True
    
    def _set_prompt(self):
        if self._dbfs:
            self.prompt = self._dbfs.prompt()
        else:
            self.prompt = "*disconnected* "
            
    def postcmd(self, stop, line):
        self._set_prompt()
        return stop

    def _parse(self, arg, *defaults, min=0, max=None):
        args = shlex.split(arg)
        max1 = max if max is not None else min + len(defaults)
        if ((len(args) < min) or
            (max1 >= 0 and len(args) > max1)):
            raise Exception("wrong number of arguments")

        args += defaults[len(args)-min:]
        
        return args
    
CLI().cmdloop()
