#!/usr/bin/env python3

import sys
import asyncio
import aiohttp
import cmd
import configparser
import os.path
import urllib.parse
import traceback
import time
import shlex
import pathlib
import base64

class FileInfo():

    @staticmethod
    def from_json(json):
        return FileInfo(json["is_dir"],
                        json["file_size"],
                        json["modification_time"],
                        json["path"])
    
    def __init__(self, is_dir, size, mtime, abspath):
        self._is_dir = is_dir
        self._size = size
        self._mtime = mtime
        self._abspath = abspath

    def path(self):
        return os.path.basename(self._abspath)

    def is_dir(self):
        return self._is_dir

    def mtime(self):
        return self._mtime

    def size(self):
        return self._size

    def type(self):
        return "dir" if self._is_dir else "file"
        
class Disconnected():
    def __init__(self):
        pass
    
    def __getattr__(self, method):
        raise Exception("open must be called first!")

    def prompt(self):
        return "*disconnected* "

class DBFS():

    def __init__(self, id, host, cluster_id, token,
                 workers=8, chunk_size=1048576, retries=10):
        self.id = id
        self.host = host
        self.cluster_id = cluster_id
        self.token = token
        self.path = "/"
        self.workers = workers
        self.chunk_size = chunk_size
        self.retries = retries
        # print(f"host: {self.host}, cluster: {self.cluster_id}, token: {self.token}")

    def check(self):
        self._assert_dir(".")
        
    def _resolve(self, path):
        return os.path.normpath(os.path.join(self.path, path))

    def prompt(self):
        return f"{self.id}:{self.path}$ "
    
    def cd(self, path):
        path = self._resolve(path)
        self._assert_dir(path)
        self.path = path

    def _simple_get(self, end_point, **params):

        async def worker():
            async with aiohttp.ClientSession() as session:
                url = urllib.parse.urljoin(self.host, end_point)
                headers = { "Authorization": "Bearer " + self.token }
                #print(f"url: {url}, headers: {headers}")
                async with session.get(url,
                                       params=params,
                                       headers=headers) as response:
                    #print("Status:", response.status)
                    #print("Content-type:", response.headers['content-type'])
                    #print("Content:", await response.text())
                    return await response.json()

                
        loop = asyncio.get_event_loop()
        return loop.run_until_complete(worker())

    def _get_status(self, path):
        r = self._simple_get("api/2.0/dbfs/get-status", path=self._resolve(path))
        return FileInfo.from_json(r)

    def ls(self, path):
        path = self._resolve(path)
        fi = self._get_status(path)
        if fi.is_dir():
            r = self._simple_get("api/2.0/dbfs/list", path=path)
            return [FileInfo.from_json(e) for e in r["files"]]
        else:
            return [fi]
    
    def _assert_dir(self, path):
        if not self._get_status(path).is_dir():
            raise Exception("Not a directory")

    def get(self, src, target):
        src = self._resolve(src)
        fi = self._get_status(src)
        url = urllib.parse.urljoin(self.host, "api/2.0/dbfs/read")
        headers = { "Authorization": "Bearer " + self.token }

        queue = asyncio.Queue(maxsize = self.workers * 2)
        ok = True

        with open(target, "wb") as out:
        
            async def slave():
                nonlocal ok
                async with aiohttp.ClientSession() as session:
                    while True:
                        task = await queue.get()
                        if task is None:
                            return
                        offset = task["offset"]
                        length = task["length"]
                        retries = 0
                        while True:
                            try:
                                async with session.get(url,
                                                       params={"path": src, "offset": offset, "length": length},
                                                       headers=headers) as response:
                                    r = await response.json()
                                bytes_read = r["bytes_read"]
                                if bytes_read == 0:
                                    raise Exception("empty data response")


                                #print(f"{bytes_read} bytes copied")
                                out.seek(offset)
                                out.write(base64.standard_b64decode(r["data"]))

                                if bytes_read < length:
                                    # we ask for the remaining data without incrementing the retries counter
                                    length -= bytes_read
                                    offset += bytes_read
                                else:
                                    # go for the next task
                                    break
                            except Exception as ex:
                                print(f"Something went wrong when transferring {length} bytes at offset {offset} ({ex})")
                                retries += 1

                                if retries > self.retries: # too many retries
                                    ok = False
                                    print("Aborting!")
                                    break
                                print(f"Delaying before retrying [{retries}]...")
                                await asyncio.sleep(1)

            async def master():
                nonlocal ok
                offset = 0
                size = fi.size()
                while ok:
                    next_offset = min(offset + self.chunk_size, size)
                    length = next_offset - offset
                    if length <= 0:
                        break
                    #print(f"Queueing offset {offset}, length {length}...");
                    await queue.put({ "offset": offset, "length": length })
                    offset = next_offset

                for _ in range(self.workers):
                    await queue.put(None)

            async def all():
                slaves = [slave() for _ in range(self.workers)]
                await asyncio.gather(master(), *slaves)

            loop = asyncio.get_event_loop()
            loop.run_until_complete(all())

        if ok:
            return fi
        raise Exception("file copy failed")


    
class CLI(cmd.Cmd):

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        cfg_fn = os.path.join(os.path.expanduser("~"), ".databrickscfg")
        self.cfg = configparser.ConfigParser()
        self.cfg.read(cfg_fn)
        print(self.cfg)
        self.dbfs = Disconnected()
        self._set_prompt()
        self.debug = True

    def _tell_error(self, msg):
        _, ex, trace = sys.exc_info()
        print(f"{msg}: {ex}")
        if self.debug:
            print("Stack trace:")
            for t in traceback.extract_tb(trace):
                print("  File: %s, Line: %d, Func name: %s, Message: %s" % (t[0], t[1], t[2], t[3]))
        
    def do_open(self, arg):
        try:
            args = self._parse(arg)
            if args:
                id, = args
            else:
                id = "DEFAULT"

            dbfs = DBFS(id,
                        host = self.cfg.get(id, "host"),
                        cluster_id = self.cfg.get(id, "cluster_id"),
                        token = self.cfg.get(id, "token"))
            dbfs.check()
            self.dbfs = dbfs
        except:
            self._tell_error(f"Unable to open {arg}")

    def do_cd(self, path):
        try:
            self.dbfs.cd(path)
        except:
            self._tell_error(f"{path}: unable to change dir")

    def _format_size(self, size):
        if (size >= 1073741824):
            return "%.1fG" % (size / 1073741824)
        if (size > 1024 * 1024):
            return "%.1fM" % (size / 1048576)
        if (size > 1024):
            return "%.1fK" % (size / 1024)
        return str(size)

    def _format_time(self, mtime):
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(mtime/1000))

    def do_ls(self, arg):
        try:
            args = self._parse(arg)
            if args:
                path, = args
            else:
                path = "."
            # cols are type, size, mtime and path
            type_len = 3
            size_len = 1
            mtime_len = 1
            table = []
            for e in self.dbfs.ls(path):
                row = (e.type(),
                       self._format_size(e.size()),
                       self._format_time(e.mtime()),
                       e.path())

                # print(f"row: {row}")
                type_len = max(type_len, len(row[0]))
                size_len = max(size_len, len(row[1]))
                mtime_len = max(mtime_len, len(row[2]))
                table.append(row)

            fmt = "{:>"+str(type_len)+"} {:>"+str(size_len)+"} {:>"+str(mtime_len)+"} {}"
            for e in table:
                print(fmt.format(*e))

        except:
            self._tell_error(f"{arg}: unable to list directory")

    def do_lcd(self, arg):
        try:
            path, = self._parse(arg)
            os.chdir(path)
        except:
            self._tell_error(f"{arg}: unable to change dir")

    def do_lpwd(self, arg):
        try:
            print(os.getcwd())
        except:
            self._tell_error("getcwd failed")

    def _create_parent_dir(self, path):
        parent, _ = os.path.split(path)
        pathlib.Path(parent).mkdir(parents=True, exist_ok=True)
            
    def do_get(self, arg):
        try:
            args = self._parse(arg)
            if len(args) < 1 or len(args) > 1:
                raise Exception("wrong number of arguments")

            src = args[0]
            target = args[1] if len(args) > 1 else "."

            if os.path.isdir(target):
                target = os.path.join(target, os.path.basename(src))
            if os.path.exists(target):
                raise Exception("file already exists")

            self._create_parent_dir(target)

            start = time.time()
            fi = self.dbfs.get(src, target)
            delta = max(1, time.time() - start)
            size = fi.size()
            kbps = size / 1024 / delta
            print(f"{size} bytes copied in {int(delta)} seconds ({self._format_size(kbps)}bps)")
                
        except:
            self._tell_error(f"{arg}: unable to retrieve remote file")
            
    def _set_prompt(self):
        if self.dbfs:
            self.prompt = self.dbfs.prompt()
        else:
            self.prompt = "*disconnected* "
            
    def postcmd(self, stop, line):
        self._set_prompt()

    def _parse(self, arg):
        return shlex.split(arg)
        
        
CLI().cmdloop()
